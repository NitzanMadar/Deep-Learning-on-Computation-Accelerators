{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 2: Summary Questions\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "[*Receptive Field*](https://en.wikipedia.org/wiki/Receptive_field), of a biological neuron is the portion of the sensory space that can elicit neuronal responses, when stimulated.\n",
    "In deep learning context, the receptive field is defined as the size of the region in the input that produces the feature.\n",
    "As shown in [tutorial 4](https://nbviewer.jupyter.org/github/vistalab-technion/cs236781-tutorials/blob/master/t04/tutorial4-CNNs.ipynb?flush_cache=true) using the figure below, in CNNs each pixel in each layer may has information from the previous layer, depending on the architecture.\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://theaisummer.com/assets/img/posts/receptive-field/receptive-field-in-convolutional-networks.png' width=300>\n",
    "<figcaption>Receptive Field Illustration</figcaption></center>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "\n",
    "We can control the receptive field size using: \n",
    "\n",
    "* **Kernel Size:** the receptive fiels is proportional to the kernel size - for a kenel of size $k$, each pixel in the next layer has information of $k$ pixels from the previous layer.\n",
    "* **Strides:**  the receptive fiels is proportional to the stride because the bigger the stride the less overlapping pixels we have. Note that, if the stride is bigger than the kernel size $k$, we will miss pixels.\n",
    "* **Dilation:** See figure (D) here, compared to all other. \n",
    "For kernel of size $k$, a dilation of factor of $d$ (stride of $d$ between pixels in the source of the convolution) will increases the receptive field to $d \\cdot (k - 1) + 1$ (because the kernel gets wider wider with holes between each adjacent kernel pixels). Therefore, for $d>1$ the region in the input that produces the feature gets bigger.\n",
    "<!-- <figure>\n",
    "<center>\n",
    "    <img src='https://miro.medium.com/max/494/0*oX5IPr7TlVM2NpEU.gif' width=300>\n",
    "    <figcaption>Dilation = 1</figcaption></center>\n",
    "<figure>\n",
    "<center>\n",
    "    <img src='https://miro.medium.com/max/494/0*3cTXIemm0k3Sbask.gif' width=300>\n",
    "        <figcaption>Dilation = 2</figcaption></center> -->\n",
    "\n",
    "| (A) No paading, Stride = Dilation = 1 | (B) Padding = Stride = Dilation = 1 | (C) Stride = 2, Padding = Dilation = 1  | (D) No padding, Stride = 1, Dilation = 2 |\n",
    "| --- | --- | --- | --- | \n",
    "|  <img src='https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif' width=200> | <img src='https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif' width=200>|  <img src='https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif' width=200>| <img src='https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif' width=200>| \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 122, 122])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "\n",
    "    \n",
    "The receptive field for 1D input can be calculated using the next recursive formula: $RF_{l-1} = s_l \\cdot (RF_l - 1) + d_l \\cdot (k_l - 1) + 1$\n",
    "where $RF_i$ is the receptive field in the layer $i$ and $RF_L=$ where $L$ is the last layer, $k_l$ is the kernel size, $s_i$ is the stride, $d_l$ is the dilation factor. Note that layers like ReLU doesn't affect the $RF$.\n",
    "    \n",
    "We assume that the padding isn't affect the $RF$ and they just used to correct edges effects, as we have here.\n",
    "The explantion for this recursive formula is:\n",
    "* the stride $s_l$ effect - is simply by miltiplygin the previous $RF_{l-1}$, and $s_l-1$ are skipped $\\longrightarrow RF_{l-1}=s_l \\cdot RF_l - (s_l - 1)$\n",
    "* The kernel size $k_l$ and dilation $d_l$ - additive $k_l-1$ that cover margins (assumint that we using the correct padding, that is $\\lfloor \\frac{k_l-1}{2} \\rfloor$). Additionaly, dilation $d_l \\neq 1$ make the the convolution to use pixels that are not adjacence, and therefore it increses the additional term we mentioned before to $d_l \\cdot (k_l - 1) + 1$\n",
    "\n",
    "    \n",
    "Summing this two terms, provided the recursive formula we write at the begging of the answer.\n",
    "Here we have image as an input, so the anwer will be equal for both aces ($RF \\times RF$).\n",
    "    Using that formula, we can recursively calculate the original $RF$ of the architecture above, which denoted here as $RF_0$:\n",
    "\n",
    "$\n",
    "\\text{ReLU layers:} \\ RF_1 = RF_2 ; RF_7 = RF_8 ; RF_4 = RF_5 \\\\\n",
    "\\text{Final layer:} \\ RF_8 = 1 \\\\\n",
    "$\n",
    "    \n",
    "$\n",
    "r_0 = 1 \\cdot (RF_1 - 1) + 1 \\cdot (3 - 1) + 1  = RF_1 + 2 = RF_2 + 2 =\\\\ \n",
    "= ( 2 \\cdot (RF_3 - 1) + 1 \\cdot (2 - 1) + 1 ) + 2 = \\dots = 2 \\cdot RF_3 + 2 =\\\\\n",
    "= 2 \\cdot ( 2 \\cdot (RF_4 - 1) + 1 \\cdot (5 - 1) + 1 ) + 2  = 4 \\cdot RF_4 + 8 = 4 \\cdot RF_5 + 8 = \\\\\n",
    "= 4 \\cdot ( 2 \\cdot (RF_6 - 1) + 1 \\cdot (2 - 1) + 1 ) + 8  = 8 \\cdot RF_6 + 8 =\\\\\n",
    "= 8 \\cdot ( 1 \\cdot (RF_7 - 1) + 2 \\cdot (7 - 1) + 1 ) + 8  = 8 \\cdot RF_7 + 104 = 8 \\cdot RF_8 + 104  \\\\ \\\\\n",
    "\\Longrightarrow RF_0 = 8 + 104 = 112\n",
    "$\n",
    "\n",
    "In conclusion, for each axis we have $RF$ of 112, and we work on image and equal operation for both axes, so we have got $RF_0=112 \\times 112$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "Basically, changing the network architecture is going to affect the weights values that optimizes the loss function.\n",
    "For example, if we use the weights learned in the original network, in the residual network sense each layer is a different function (becuase we are adding $x$ to the function) obviously we will get the wrong output. Therefore, the weights which optimize the residual network has to be adjusted to the fact that the blocks are now different (residual).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:17.992615Z",
     "iopub.status.busy": "2021-01-26T09:18:17.991868Z",
     "iopub.status.idle": "2021-01-26T09:18:18.013482Z",
     "shell.execute_reply": "2021-01-26T09:18:18.014164Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "p1, p2 = 0.1, 0.2\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=p1),\n",
    "    nn.Dropout(p=p2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to replace the two consecutive dropout layers with a single one defined as follows:\n",
    "```python\n",
    "nn.Dropout(p=q)\n",
    "```\n",
    "what would the value of `q` need to be? Write an expression for `q` in terms of `p1` and `p2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "\n",
    "The `nn.Dropout(p)` layer randomly zeroes some elements of the input tensor with probability $p$ using samples from a Bernoulli distribution. Therfore after sequantial of `nn.Dropout(p=p1)` $\\rightarrow$ `nn.Dropout(p=p2)` an element will zero if it zeroed in the first dropout layer $(p_1)$ <font color='red'>**OR**</font> in the second dropout layer $(p_2)$ (remember that they statistically independent).\n",
    "Therefore, an equivalent layer `nn.Dropout(p=q)` will need to be with probability $q = Pr(dropout_1 \\cup dropout_2) = p_1 + p_2 - p_1 \\cdot p_2 = 0.1 + 0.2 - 0.1 \\cdot 0.2 = 0.28$.\n",
    "    \n",
    "Simplier way to think of it calculate it as a complementary event, i.e., the probability of neuron to survive is a dropout layer is $1-p$ so what we need is the complementary even of both $(1-p_1)$<font color='red'> **AND** </font> $(1-p_2)$ and hence the equivalent is: $q = 1-Pr(\\overline{dropout}_1 \\cap \\overline{dropout}_2) = 1-(1-p_1) \\cdot (1-p_2) = 1-0.9 \\cdot 0.8 = 0.28$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "False. Usually, the dopout is applied after the activation function, but it is not a must. For example, considering the ReLU activation function, it makes more sense to apply the dropout before because $ReLU(0) = 0$, so it more computationally efficient. However, it is not a necessary.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "The dropout activation can be written as:\n",
    "$$\n",
    "    y_{dropout} = f(x)= \n",
    "\\begin{cases}\n",
    "    x,              & \\text{with probability of} \\ 1-p \\\\\n",
    "    0,              & \\text{with probability of} \\ p\n",
    "\\end{cases}\n",
    "$$\n",
    "    \n",
    "Let's calculate the expectation value $\\mathbb{E}[y_{dropout}] = p\\cdot 0 +(1-p) \\cdot x = (1-p) \\cdot x $\n",
    "It easy to see that without the drop we have $\\mathbb{E}[y_{without dropout}] = x$\n",
    "and we got a scale factor of: $\\frac{\\mathbb{E}[y_{without dropout}]}{\\mathbb{E}[y_{dropout}] } = \\frac{1}{1-p} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "We wouldn't choose L2 loss in this case, bacause it won't strongly penelize for wrong classification. We would choose a binary log cross entropy instead.\n",
    "    \n",
    "We will explain it with a numerical example: let assume that our classifier is completly wrong to classify a hotdog (1), that means it predict 0 (dog). So, the L2 loss will be $(1-0)^2 = 1$, where the binary cross entropy will be $- (0 \\cdot log(1) + 1 \\cdot log(0)) \\longrightarrow \\infty$.\n",
    "\n",
    "In addition, using MSE means that we assume that the underlying data has been generated from a normal distribution. In Bayesian terms this means we assume a Gaussian prior. While in reality, a dataset that can be classified into two categories (i.e. binary) is not from a normal distribution but a Bernoulli distribution.\n",
    "Besides, the MSE function is non-convex for binary classification. In simple terms, if a binary classification model is trained with MSE Cost function, it is not guaranteed to minimize the Cost function. This is because MSE function expects real-valued inputs in range($-\\infty$, $\\infty$), while binary classification models output probabilities in $[0,1]$ through the sigmoid/logistic function. \n",
    "    \n",
    "<!-- <font color='red'>\n",
    "===========\n",
    "IMO No. https://towardsdatascience.com/why-using-mean-squared-error-mse-cost-function-for-binary-classification-is-a-bad-idea-933089e90df7 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://sparrowism.soc.srcf.net/home/piratesarecool4.gif\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H), nn.Sigmoid(),\n",
    "    ]*24,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")\n",
    "\n",
    "# print(mlpirate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "We assume that the most likely cause for this is the [\"Vanishing Gradients\" problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), that happens when the gradients calculated during backpropagation are small and that can make the neuron to have no change in the values.\n",
    "It make sense that this is the problem due to the fact that model is too deep and has a lot of sigmoid layers (25*(FC->Sigmoid)->FC). As we can see at the figure added below, the sigmoid gradient is $\\in [0, \\sim 0.25]$, so it make sense why the multipication of that 25 times will causes the gradients to vanish.\n",
    "    \n",
    "<figure>\n",
    "<center>\n",
    "    <img src='https://miro.medium.com/max/3000/1*6A3A_rt4YmumHusvTvVTxw.png' width=800>\n",
    "        <figcaption>Sigmoid and derivative of sigmoid plots</figcaption></center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "No, he is wrong. Although the $tanh$ derivative is $\\in [0,1]$ which is better, it still has gradients that less than $1$ value, and the fact that the network architecture is still too deep and the vanishing gradient problem will not be solved.\n",
    "    \n",
    "<figure>\n",
    "<center>\n",
    "    <img src='https://qph.fs.quoracdn.net/main-qimg-f1baf29cfdb09202b18e2179f4f41bfc' width=500>\n",
    "        <figcaption>Sigmoid and derivative of sigmoid plots</figcaption></center> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "  1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "  1. The gradient of ReLU is linear with its input when the input is positive.\n",
    "  1. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "A. True. The deriviative of ReLU is\n",
    "    $\n",
    "    \\frac{d}{dx}ReLU(x) = \\frac{d}{dx}max(0,x) = \n",
    "\\begin{cases}\n",
    "    1,              & x>0 \\\\\n",
    "    0,              & x \\leq 0  \\ \\text{($x=0$ is undefined but set to 0))}\n",
    "\\end{cases}\n",
    "$ <br>\n",
    " Vanishing gradint can happens only when the derivative of the activation function is $\\in (0, 1)$, because we can get kind of geomeric seriest which convergence to 0.\n",
    "    \n",
    "B. True. As mention in the previous section, for positive input the gradient is constantly $1$, so it is linear for positive input $x$.  \n",
    "    \n",
    "C. True. It is possible since negative inputs ReLU will outputs $0$ that can make the activation remain a value of $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "<figure>\n",
    "<center>\n",
    "    <img src='https://suniljangirblog.files.wordpress.com/2018/12/descent.png' width=500>\n",
    "    <figcaption>GD, SGD and mini-batch SGD visualization</figcaption></center>\n",
    "<figure><\\center>\n",
    "\n",
    "<!-- https://suniljangirblog.wordpress.com/2018/12/13/variants-of-gradient-descent/\n",
    "     -->\n",
    "    \n",
    " \n",
    "All the GD methods are iterative methods, which updates the parameters in a way that reduces the function value untill we reach a minimum.\n",
    "* **GD:** In GD, we work with all the samples in out training in every run, the gradient of the loss function is computed exactly in each step.\n",
    "* **SGD:** This is an approximation of GD. The gradient of the loss function is approximated by a gradient of a random single sample (that doesn't choosed before).\n",
    "* **Mini-batch SGD:** the gradient of the loss function is approximated by a gradient of a batch of samples. In other words, this method uses principles from both GD (use batch) and SGD (use part of the data rather than all of it).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding SGD and GD:\n",
    "  1. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "  2. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "A. SGD is more often used in practice becuase:\n",
    "\n",
    "1. In order to use GD we need to use **all** the training set for each update of weights. if the training set is large, it can take a long runtime to reach the mininum by using GD.\n",
    "    \n",
    "2. Sometimes the training set is too large to even fit into the memory (all at once). In this case we can not use GD.\n",
    "    \n",
    "B. In the case where the training data is too big to fit in memory (all at once) we can not use GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "We expect that the number of iterations it takes to converge to $l_0$ will decrease - we will converge faster.\n",
    "Basically, as a rule of thumb, larger batch size create more robust and accurate model.\n",
    "It can be seen from the image in question #1 in this section, that increasing the batch size will decrease the number of iterations to convergence.\n",
    "Since the ideal proccess is GD - which uses the whole data it converges the fastest, increasing the batch size will get us closer to the ideal proccess, and therefore we will converge faster.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "  1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "  1. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "  1. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "  1. Training  with SGD requires more memory than with GD.\n",
    "  1. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "  1. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "        \n",
    "A. True - SGD using randomly one sample and avoid repeating the same sample, thus it iterate over the whole dataset in each epoch. Sometimes, this process is described by shuffling the whole database every epoch and using samples one after the other by the predefined order.\n",
    "    \n",
    "<!--     ####NOT SURE#### False - In SGD every iteration we choose stochastically one sample, and update the parameters taking into consideration only this one sample and it is chosen stochastically.\n",
    "So after one epoch we might use the same sample a couple of times, or not use a sample at all.\n",
    "     -->\n",
    "B. False - Exactly the opposite. Since it uses only one example from the data, the update process is vety noisy with high variance. In the expectation SGD converges to GD, but when acctually performing SGD we might take steps \"the wrong way\", but on average we are progressing towards the minumum. so the variance is greater in SGD.\n",
    "    \n",
    "C. True - If we are currently in a local minima, GD is stuck, because when using GD we compute the exact gradient of the function, which is 0. But, SGD might get itself out of the local minima, since it uses only some of the sample and not the whole data set. So the gradient is not exatly 0, so if we keep iterating we can escape this local minima. In other words, the noise we described in section B here, might help the SGD in this case to get out of the local minima.\n",
    "    \n",
    "D. False - Using GD we need all our dataset for each iteration. Using SGD we just use a minibatch (or maybe even one sample) each iteration.\n",
    "    \n",
    "E. False - Both methods guarantees to converge only to a local minima. If the funciton is not convex the minimal value we converge to some minima depends on the initialization of the parameters - and it may be a local minima. \n",
    "    \n",
    "F True - Using momentum can help in this case because the momentum accumulates the gradients from previous iterations (from the beginning until current step with decay factor), if the gradient in current iteration has the same direction as the momentum, then the weight-update in the same direction will have larger size step and thus in our case it will coverge faster. See visuallization below.\n",
    "    \n",
    "<figure>\n",
    "    <center>\n",
    "        <img src='https://media.springernature.com/lw785/springer-static/image/chp%3A10.1007%2F978-1-4842-4470-8_33/MediaObjects/463852_1_En_33_Fig1_HTML.jpg' width=400>\n",
    "        <figcaption>The effect of using momentum in SGD method</figcaption>\n",
    "    </center>\n",
    "<figure>\n",
    "    \n",
    "    \n",
    "    \n",
    "<!-- *in order to use newton's method, we need to caculate the hessian matrix which has a dimention of $n*n$ where n is the number of parameters.\n",
    "   \n",
    "*in order to use SGD with momentum, we only need to calculate the gradient of the current step, and accumulate gradients from the past steps.\n",
    "Furthermore, the fact that we use the momentum, will enlarge the gradient in the direction of the ravine.\n",
    "These are the reasons that SGD with momentum will converge faster. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In tutorial 5 we saw an example of bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "  **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "False - \n",
    "    As shown in the tutorial, we want to find \n",
    "    $\\tilde{y}$ such that: \n",
    "    $$\\tilde{y}=argmin_y f(y;z)$$\n",
    "    \n",
    "We also know the following:\n",
    "    \n",
    "* If the gradient of $f$ is $0$ for a given $y$, the perturbing $z \\rightarrow z+dz$ will \"move\" the minimum to different $y+dy$.\n",
    "    \n",
    "* Therefore, we can write using Taylor: \n",
    "$$\\nabla_y f(y+dy,z+dz) \\approx \\nabla_y f(y,z)+\\nabla^2_{yy} f(y,z)dy+\\nabla^2 _{yz} f(y,z)dz=0 $$\n",
    "    \n",
    "* because: \n",
    "$$\\nabla_y f(y,z)=0$$\n",
    "    we obtain: \n",
    "    $$\\nabla^2_{yy} f(y,z)dy=-\\nabla^2 _{yz} f(y,z)dz$$\n",
    "and then we can write:\n",
    "    $$dy =-[\\nabla^2_{yy}f(y,z)]^{-1}\\nabla^2 _{yz}f(y,z)dz $$\n",
    "So we got the term we need in order to calculate the gradient without a decent based method.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "  1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "  2. How can each of these problems be caused by increased depth?\n",
    "  3. Provide a numerical example demonstrating each.\n",
    "  4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "A. vanishing gradients is a case where the gradient are very close to 0. \n",
    "    exploading gradients is a case where the gradient are getting bigger and bigger and approach infinity. \n",
    "    In either way it is hard to learn.\n",
    "    \n",
    "B. In the back propagation algorithms we multiplay the gradients. the number of gradient which are being multiplyed is proportion to the number of layers.\n",
    "    if the gradients are less then one and the number of layers is big. then they will vanish (it is a geometric series with q less then 1) - vanishing gradients.\n",
    "    if the gradients are greater then one and the number of layers is big. then they will explode (it is a geometric series with q greater then 1) - exploading gradients.\n",
    "    \n",
    "C. Lets consider a case where we have K hidden layers.\n",
    "    Therefore the update for the first layer is: $\\pderiv{L}{W_1} = \\pderiv{L}{h^k} \\cdot \\pderiv{h^k}{h^{k-1}} \\cdot \\pderiv{h^(k-1)}{h^{k-2}} ... \\pderiv{h^(1)}{W_1}$\n",
    "    We have K+1 terms here if each term here is less than 0.1 and $q<1$, then we get: $\\pderiv{L}{W_1} < 0.1^{K+1}$ which where K is big it converges to 0 - vanishing gradients.\n",
    "    if each term here is greater than 1.1 and $q<1$, then we get: $\\pderiv{L}{W_1} < 1.1^{K+1}$ which where K is big it converges to infinity - exploading gradients.\n",
    "    \n",
    "D. We can identify the case by looking at the lose funciton:\n",
    "    if the loss is stationary we have vanishing gradiants.\n",
    "    if the loss is changing rapidly (in each iteration) we have exploading gradients.\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y,\\hat{y}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "* w.r.t. $W_1$:\n",
    "$$\\frac{\\partial L_s}{\\partial W_1} =\\frac{1}{N} \\sum \\frac{\\partial l}{\\partial W_1} + \\lambda ||W_1||$$\n",
    "Where the following stands:\n",
    "$$\\frac{\\partial l}{\\partial W_1} = -y \\cdot \\frac{\\partial log(\\hat{y})}{\\partial W_1} - (1-y) \\frac{\\partial log(1-\\hat{y})}{\\partial W_1}$$\n",
    "$$\\frac{\\partial log(\\hat{y})}{\\partial W_1} = \\frac{1}{\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial W_1}$$\n",
    "$$\\frac{\\partial log(1-\\hat{y})}{\\partial W_1} = -\\frac{1}{1-\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial W_1}$$\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial W_1} = W_2 \\phi'(W_1x+b_1)x^t $$\n",
    "* w.r.t. $W_2$:\n",
    "$$\\frac{\\partial L_s}{\\partial W_2} =\\frac{1}{N} \\sum \\frac{\\partial l}{\\partial W_2} + \\lambda ||W_2||$$\n",
    "Where the following stands:\n",
    "$$\\frac{\\partial l}{\\partial W_2} = -y \\cdot \\frac{\\partial log(\\hat{y})}{\\partial W_2} - (1-y) \\frac{\\partial log(1-\\hat{y})}{\\partial W_2}$$\n",
    "$$\\frac{\\partial log(\\hat{y})}{\\partial W_2} = \\frac{1}{\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial W_2}$$\n",
    "$$\\frac{\\partial log(1-\\hat{y})}{\\partial W_2} = -\\frac{1}{1-\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial W_2}$$\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial W_2} = \\phi(W_1x+b_1) $$\n",
    "    \n",
    "* w.r.t. $b_1$:\n",
    "$$\\frac{\\partial L_s}{\\partial b_1} =\\frac{1}{N} \\sum \\frac{\\partial l}{\\partial b_1}$$\n",
    "Where the following stands:\n",
    "$$\\frac{\\partial l}{\\partial b_1} = -y \\cdot \\frac{\\partial log(\\hat{y})}{\\partial b_1} - (1-y) \\frac{\\partial log(1-\\hat{y})}{\\partial b_1}$$\n",
    "$$\\frac{\\partial log(\\hat{y})}{\\partial b_1} = \\frac{1}{\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial b_1}$$\n",
    "$$\\frac{\\partial log(1-\\hat{y})}{\\partial b_1} = -\\frac{1}{1-\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial b_1}$$\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial b_1} = W_2 \\phi'(W_1x+b_1)  $$\n",
    "\n",
    "* w.r.t. $b_2$: \n",
    "$$\\frac{\\partial L_s}{\\partial b_2} =\\frac{1}{N} \\sum \\frac{\\partial l}{\\partial b_2}$$\n",
    "Where the following stands:\n",
    "$$\\frac{\\partial l}{\\partial b_2} = -y \\cdot \\frac{\\partial log(\\hat{y})}{\\partial b_2} - (1-y) \\frac{\\partial log(1-\\hat{y})}{\\partial b_2}$$\n",
    "$$\\frac{\\partial log(\\hat{y})}{\\partial b_2} = \\frac{1}{\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial b_2}$$\n",
    "$$\\frac{\\partial log(1-\\hat{y})}{\\partial b_2} = -\\frac{1}{1-\\hat{y}} \\frac{\\partial \\hat{y}}{\\partial b_2}$$\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial b_1} =\\vec{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Derivative a function $f(\\vec{x})$ at a point $\\vec{x}_0$ is\n",
    "  $$\n",
    "  f'(\\vec{x}_0)=\\lim_{\\Delta\\vec{x}\\to 0} \\frac{f(\\vec{x}_0+\\Delta\\vec{x})-f(\\vec{x}_0)}{\\Delta\\vec{x}}\n",
    "  $$\n",
    "  \n",
    "  1. Explain how this formula can be used in order to compute gradients of neural network parameters numerically, without automatic differentiation (AD).\n",
    "  \n",
    "  2. What are the drawbacks of this approach? List at least two drawbacks compared to AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "<!-- <font color='red'>\n",
    " -->\n",
    "A. We can take a small number $\\Delta\\vec{W}$ and calculate the function value in $\\vec{W}_0$ and $\\vec{W}_0+\\Delta\\vec{W}$ (where $\\vec{W}$ stands for a paramerter of NN) and compute the value of the function above. That would be the numerical derivative of the NN with respect to $\\vec{W}$.\n",
    "    We can do this calculation for each parameter $\\vec{W}$ in the NN and get the gradient vector numerically without automatic differentition.\n",
    "\n",
    "    \n",
    "    \n",
    "B. Two drawback are:\n",
    "    \n",
    "1. Accuracy, this  $\\Delta\\vec{W}$ we take will be finite, and not actually approaching to 0. Therefore this method will have an approximation error.\n",
    "2. This method is an approximation and another drawback might be that it can produces a graident where the function is not differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Given the following code snippet:\n",
    "  1. Write a short snippet that implements that calculates gradient of `loss` w.r.t. `W` and `b` using the approach of numerical gradients from the previous question.\n",
    "  2. Calculate the same derivatives with autograd.\n",
    "  3. Show, by calling `torch.allclose()` that your numerical gradient is close to autograd's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.5318, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, d = 100, 5\n",
    "dtype = torch.float64\n",
    "\n",
    "X = torch.rand(N, d, dtype=dtype)\n",
    "W, b = torch.rand(d, d, requires_grad=True, dtype=dtype), torch.rand(d, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def foo(W, b):\n",
    "    return torch.mean(X @ W + b)\n",
    "\n",
    "loss = foo(W, b)\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "# TODO: Calculate gradients numerically for W and b\n",
    "epsilon = 0.00001\n",
    "W_1 = torch.clone(W)\n",
    "b_1 = torch.clone(b)\n",
    "\n",
    "\n",
    "grad_W = torch.zeros(d, d, dtype=dtype)\n",
    "grad_b = torch.zeros(d, dtype=dtype)\n",
    "for i in range(d):\n",
    "    for j in range(d):\n",
    "        W_1[i][j] = W[i][j] + epsilon\n",
    "        grad_W[i][j] = (foo(W_1,b) - (foo(W,b)))/epsilon\n",
    "        W_1[i][j] = W[i][j]\n",
    "\n",
    "for i in range(d):\n",
    "        b_1[i] = b[i] + epsilon\n",
    "        grad_b[i] = (foo(W,b_1) - (foo(W,b)))/epsilon\n",
    "        b_1[i] = b[i]\n",
    "\n",
    "# TODO: Compare with autograd using torch.allclose()\n",
    "loss.backward()\n",
    "autograd_W = W.grad\n",
    "autograd_b = b.grad\n",
    "\n",
    "assert torch.allclose(grad_W, autograd_W)\n",
    "assert torch.allclose(grad_b, autograd_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding word embeddings:\n",
    "  1. Explain this term and why it's used in the context of a language model.\n",
    "  1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding? If yes, what would be the consequence for the trained model? if no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "\n",
    "A. word embeddings, is the representation of the word in a meaningful way (a vector), that encodes the meaning of the words in such a way that it can identify similarities between 2 words, by their representation in the vector space.\n",
    "\n",
    "B. It can train, but the results would be poor, since it would not be able to identify the context of the words, moreover, it would not be able to identify the similaritier\\unsimilarities between differrent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain:\n",
    "  1. What does `Y` contain? why this output shape?\n",
    "  2. How you would implement `nn.Embedding` yourself using only torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "A.num_embeddings is the number of words in the vocab.\n",
    "    embedding_dim is the dimention of words vector we are using.\n",
    "    So Y will be equal to the words imbedding which is given by the indices of X.\n",
    "    The shape of Y is the same as X multiplied by the embedding dimention, because it replaces each index with a words. so 5x6x7x8 goes to (5x6x7x8)x42000.\n",
    "    \n",
    "B. As i said earlier num_embeddings is the number of words in the vocab.\n",
    "    embedding_dim is the dimention of words vector we are using.\n",
    "    So I would create a NN with an imput dimention of 42, and output dimention of 42000. every index (a number between 0-42) will be mapped to a vector which is 42000 dimentional.\n",
    "    the mapping will be learned by training.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of S: State whether the following sentences are **true or false**, and explain.\n",
    "  1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "  2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length S.\n",
    "  3. TBPTT allows the model to learn relations between input that are at most S timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "A. True - The only modification to backpropagation, will be to accumulate gradients on in length S. since thats the only \" short term memory\" we want.\n",
    "    \n",
    "B. False - we don't have to limit the sequence length, we can just limit the number of timesteps of the backpropagation algorithm to length S.\n",
    "    \n",
    "C. True - since we limit the gradient accumulation to length S, we would only have \"memory\" of at most S timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 7 (part 2) we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "  1. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention\n",
    "  \n",
    "  2. After learning that self-attention is gaining popularity thanks to the shiny new transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections). What influence do you expect this will have on the learned hidden states?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "<font color='red'>\n",
    "    \n",
    "A. Without attention the decoder recieves the hidden state from the encoder as is.\n",
    "    By adding attention, the decoder will focus on differente parts of the sequence.\n",
    "    The attention (soft attention mechanism) is a weighted average of the encoder ouputs, that match the current decoder state.\n",
    "\n",
    "    \n",
    "    \n",
    "B. In this case the hidden states of the decoder are not used - so they won't be learned.\n",
    "   What is going to happen, is that the hidden states of the encoder will change - they will be similar to the meaning of the sentence. \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "  1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "  1. Images generated by the model ($z \\to x'$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "A. in order to reconstract the image, we only need to use both the encoder and decoder, therefore we need the KL-divergence term (which is responsible for encoding) and the reconstruction term (which is responsibale for decoding) optimized.\n",
    "    \n",
    "B. Here the fact that we forgot to include the KL-divergence term is not going to damage us,because optimizing the KL-divergence term is responsible for the encoder, and not the decoder which generates samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  2. Every time we feed an image to the encoder, then decode the result, we'll get the same reconstruction.\n",
    "  3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "A. TRUE - thats the way we train the model.\n",
    "\n",
    "B. FALSE - There is a sampling envolved in this proccess therefore it is not deterministic.\n",
    "\n",
    "C. TRUE - we optimize the KL-divergence term by optimizing the ELBO - evidence lower bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "  2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "  3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "  5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "    \n",
    "A. False. We want both of the disctriminator and the generator to work good and have a low losses. due to the fact that they train one against the other, we want them to have \"fair competition\". In other words, it will be hard to make a good discriminator (low loss) to have wrong decision and hence the generator will need to do a better job.\n",
    "    \n",
    "B. False. they are being train seperatly, so when we train the discriminator, we use as input the generator's output and we do not train it.\n",
    "    \n",
    "C. True. The generator maps a latent-space variable $u\\sim \\mathcal{N}(0, I)$ to instance-space varibale $x$, which is an image. Therefore a  parametric evidence distribution $(p_\\gamma(X))$ is generated and we try to make it as closer as possible to the real evidence distribution $p(X)$.\n",
    "    \n",
    "D. False. At the beginning of the process, both the discriminator and the generator has bad performence, and hence it won't help us to have a better discriminator at the first steps. In addition, to create a good trainning for the discriminator, we need also good \"fake\" images, which need to be created for the untrained generator, thus it doesn't help us to train the discriminator for few epoch before.\n",
    "###THIS IS THE REF,WHAT DO U WANNA DO?### True, we would want a decent discriminator in order to achieve better training of the generator.\n",
    "    \n",
    "E. False. If the discriminator has 50% accuracy, the generator will not be able to learn how to generate, because effectivly the discriminator is \"helping\" the generator to generate, so there is no use in training only the generator in this case.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have implemented a graph convolutional layer based on the following formula, for a graph with $N$ nodes:\n",
    "$$\n",
    "\\mat{Y}=\\varphi\\left( \\sum_{k=1}^{q} \\mat{\\Delta}^k \\mat{X} \\mat{\\alpha}_k + \\vec{b} \\right).\n",
    "$$\n",
    "  1. Assuming $\\mat{X}$ is the input feature matrix of shape $(N, M)$: what does $\\mat{Y}$ contain in it's rows?\n",
    "  1. Unfortunately, due to a bug in your calculation of the Laplacian matrix, you accidentally zeroed the row and column $i=j=5$ (assume more than 5 nodes in the graph).\n",
    "What would be the effect of this bug on the output of your layer, $\\mat{Y}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:** \n",
    "\n",
    "1. A. As we saw in tutorial 10:\n",
    "$$y^{j} = \\varphi\\left( \\sum_{i=1}^{m} \\sum_{k=1}^{q} \\alpha_{k}^{ij}\\Delta^{k}x^{i}+b^{j} \\right)$$\n",
    "\n",
    "B. By looking at the formula above, if $x^{5} = 0$ we get:\n",
    "$$y^{5} = \\varphi\\left( b^{5} \\right)$$\n",
    "And the rest is according to the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We have discussed the notion of a Receptive Field in the context of a CNN. How would you define a similar concept in the context of a GCN (i.e. a model comprised of multiple graph convolutional layers)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgreen'>\n",
    "    \n",
    "**Answer:**\n",
    "\n",
    "\n",
    "2. In GCN, the most intuative way to define the concept of receptive field (for a target node), is their whole L-hop neighbors, where L is the number of layers.\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
